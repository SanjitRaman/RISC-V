# Individual Statement: Sanjit

### Contents

- [Single Cycle](#single-cycle)
    - [Google Tests](#1-google-tests)
    - [Makefile](#2-makefile)
    - [CI pipeline & Linux Server](#3-ci-pipeline--linux-server)
    - [Control Unit](#4-control-unit)
    - [Sign Extend](#5-sign-extend)
    - [RISC-V Top Level](#6-risc-v-top-level)
    - [Program Testbench](#program-testbench)
- [Pipelining](#pipelining)
    - [New top level schematic](#new-top-level-schematic)
    - [Design Review of the Hazard Unit](#design-review-of-the-hazard-unit)
    - [Updated top-level](#updated-top-level)
    - [Integration Testing](#integration-testing)
- [Cache](#cache)




## Single Cycle

### 1. Google Tests
As explained in the [joint statement](/statements/readme.md#deciding-on-testing-methodology) we used Google Tests for our testing framework. I was responsible for setting up the testing environment and writing the majority of the tests. I also helped debug the tests and integrate them into our CI pipeline.

During my UKESF internship with Siemens UltraSoc team I had used UVM and SVUnit for verification. 

Given the constraints of using Verilator, we were limited in our options for testing. Industry standard options such as UVM, SVUnit, Cocotb, and SVA (SystemVerilog Assertions) were not feasible due to compatibility issues with Verilator.

In light of these constraints, we decided to use Google Tests (gtests), a C++ testing framework supported by Verilator. I was instrumental in this decision and the subsequent implementation of gtests in our project.

#### Overview of work: 
| | | | |
|-|-|-|-|
| ![image](/images/gtest_class_definition_and_testf_example.png) | ![image](/images/control_unit_test_results_excerpt.png) | ![image](/images/control_unit_line_coverage.png) | ![image](/images/lbu_waveform.png)|
|Overwriting GoogleTest class with methods for setup to instantiate the DUT, and delete the DUT on teardown, as well as a clock_ticks function, and methods to assert the value of the output register. Then the ADDI TEST_F assembles the program and uses the class functions to test the instruction behaviour. | Control unit gtests | Coverage report generated by genhtml/LCOV from gtests for control_unit | Sample top-level behaviour of a lbu single-instruction test |


#### Sources:
To get started, I leveraged a [GitHub repository](https://github.com/mortenjc/systemverilog) that integrates Google Tests with Verilator. This served as a valuable resource in setting up our testing environment.


#### Implementation:
I developed a comprehensive set of Google Tests for each module in our design. These tests were crucial in verifying the functionality of individual modules before integrating them into the larger system. Once we were confident in the performance of each module, I created another set of Google Tests to test the entire top-level schematic. These tests ran one instruction at a time and evaluated the behavior of the CPU.


#### Learning Outcomes:
Through my contributions to Google Tests, I helped ensure the reliability and correctness of our RISC-V processor design. This experience has further strengthened my skills in test-driven development and system verification.

#### Further Improvements

For the single-instruction tests, they are written in a way that would not be easy to verify a pipelined CPU. This is because the tests are written based on the output after each clock cycle, not accounting for stalling and forwarding. To improve this, I could look at tests that are based on the output after the entire instruction has been executed.

Additionally, most testbenches have got a model of the DUT, which is used to verify the output of the DUT. To implement a UVM methodology, I would do constrained random input testing in each TEST_F, and output the coverpoints to a file. Then I would use a coverage tool to check the coverage of the DUT.


### 2. Makefile
As explained in the [joint statement](/statements/readme.md#makefile), I was responsible for writing a makefile that incorporated a variety of features required for our project.

#### Sources:

I used the makefile in [mortenjc's GitHub Repository](https://github.com/mortenjc/systemverilog) makefile, making use of the clean, runtest, genhtml targets. However I had to rewrite the makefile to support a recursive folder structure, which was quite challenging. 

Furthermore, I used the makefile from the [Project Brief](https://github.com/EIE2-IAC-Labs/Project_Brief/blob/main/reference/Makefile) for assembling programs written in assembly, integrating the target into my own makefile. 

#### Implementation:
For the `GTEST` flags, I used the `GTEST=1` flag to conditionally include the `GTEST_FLAGS`. This allows GTEST to be turned on and off easily on testbenches that do not require it.

For the `VBUDDY` flag, I had to lookup the [verilator documentation](https://verilator.org/guide/latest/exe_verilator.html?highlight=d#cmdoption-D-var) to find out how to pass variables of the makefile as preprocessor symbols to the C++ testbench. I found that I have to use the `-D` flag to pass the variable to the testbench, and then use `ifdef` to check if the variable is defined. This allowed me to conditionally output to the VBuddy peripheral.

Similarly for the `SINGLE_INSTRUCTION_TESTS` flag.

Furthermore, there is a `RUN=unit` which tests the whole RISC-V CPU and `RUN=module` to test individual modules (e.g. `sign_extend`) or groups of modules (e.g. `data_mem_wrapper`).

#### Learning Outcomes:

I learnt how makefiles work, and how to use them to automate the build process. I agreed with [John Wickerson's summary of makefiles](https://www.youtube.com/watch?v=YjXfjdCmfD4)

### 3. CI pipeline & Linux Server

Although not directly related to Instruction Architectures, in order to ensure that stable code is pushed to the main branch, I setup a CI pipeline using GitHub Actions. Once again this was a new skill to learn.

#### Sources:

I needed to know how to write a GitHub Actions workflow, so I used the [GitHub Actions documentation](https://docs.github.com/en/actions/learn-github-actions) to learn how to write a workflow in yaml that runs when a pull request is made to the main branch.

#### Implementation:

Using my knowledge from Software Systems module, I was interested in setting up my own remote linux server (which is an old laptop from the university) that I can write my code in and run regressions on as it would always be on and have only the required packages installed. Hence I needed to know how to setup a locally hosted runner in github actions. I used the [GitHub Actions documentation](https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/adding-self-hosted-runners) to learn how to setup a locally hosted runner.

This involved installing the runner on my linux server as an always-on-service, and then registering the runner with my repository. 

#### Images:

||||
|-|-|-|
|![image](/images/githhub-actions-yaml-file.png)|![image](/images/github-actions-workflow-running.png)|![image](/images/github-actions-module-test-logs-visible.png)|
|I wrote a yaml file to run module testbenches and the risc-v testbenches|Upon a push to master the build running on my remote linux server is visible live on GitHub. |Logs of each module test are visible too.|



### 4. Control Unit
As mentioned in the [joint statement](/statements/readme.md#deciding-the-top-level-schematic-control-paths-for-each-instruction-type) we were jointly responsible for defining the top level schematic and control paths for each instruction type. Once we had decided on the top level schematic, I wrote the control unit to control the datapath using `case` statements.

#### Sources:
For the ALU decoder, I used the model in [Lecture 7: Microarchitecture of RISC-V](http://www.ee.ic.ac.uk/pcheung/teaching/EIE2-IAC/Lecture%207%20-%20Microarchitecture%20(slides).pdf) to implement a base set of ALU instructions. 
- I then had to work with Sriyesh to decide upon the ALUControl signals for the rest of the instructions and codify this into our [microarchitecture specification](/rtl/alu/readme.md#aluresult)

#### Relevant Commits:
- [Implemented alu_decoder and main_decoder](https://github.com/SanjitRaman/Team-10-RISC-V/commit/f13d5b74fcedd5e28bb2e639e37c49fe027c1105)

- [Implemented control_unit.sv](https://github.com/SanjitRaman/Team-10-RISC-V/commit/5ceecce560c3eeb4d1180850b890786e653e67d5)

Because at this we were still introducing ourselves to googletest, I wrote the control unit testbench as well (breaking best practice, to show others how googletest can be used), though this was later reviewed and refactored by Arav during the debugging phase.
- [Functionalized Control Unit Testbench](https://github.com/SanjitRaman/Team-10-RISC-V/commit/4abafc618f17962a870e0abbf8effaa21069b950)
- [Finished Control Unit Testbench](https://github.com/SanjitRaman/Team-10-RISC-V/commit/4c12dab58d33a14bff9fd7d4171d34148ecc5d41)

### 5. Sign Extend

I was responsible for writing the sign extend module in Lab4-Reduced RISC-V, for which I had only implemented I-type and S-type instructions referring to [Lecture 7 Slide 13](http://www.ee.ic.ac.uk/pcheung/teaching/EIE2-IAC/Lecture%207%20-%20Microarchitecture%20(slides).pdf).

Then I had to include handling of more types of instruction formats to reach the final [design specification](/rtl/sign_extend/readme.md#overview) of sign_extend. I had to coordinate with the control unit development as well as the ALU development (Sriyesh) on this to ensure that the concatenation of the immediate is consistent with the ALU input. Note: this was done with Sriyesh.

#### Relevant Commits:
- [Initial Sign Extend Implementation](https://github.com/SanjitRaman/Team-10-RISC-V/commit/befa12b535dcfd25fc4bc40e715bc37a7c500822)
- [Additional Sign Extend Implementation](https://github.com/SanjitRaman/Team-10-RISC-V/commit/b36a111d13e47d3be071c7d2a6f33d4108ac6d1e), done by Sriyesh and I.



### 6. RISC-V Top Level

I designed the top level schematic for the RISC-V Single Cycle CPU, adapting the [microarchitecture lecture 7](http://www.ee.ic.ac.uk/pcheung/teaching/EIE2-IAC/Lecture%207%20-%20Microarchitecture%20(slides).pdf) to our project's requirements. I used ISSIE to do this in a visual way for our group to refer to.

![top level schematic](/images/single-cycle-schematic.png)

I then wrote the top level schematic in System Verilog, improving on the Lab4 design by Arav (see relevant commit)

#### Relevant Commits:
- [Implemented top level schematic in SystemVerilog](https://github.com/SanjitRaman/Team-10-RISC-V/commit/eab305e80771e4cd6573e01ebc7dcc92377cf0f6)


#### Program Testbench

In order to get the sample programs displaying on VBuddy, I wrote a [program testbench](/testbench/risc_v/versions/risc_v_full_program_tb.h). This required some back and forth with the makefile and Arav who was deugging the waves on GTKWave to find the program counter value at which we should start plotting the probability distribution function.

#### Single Instruction tests:
I setup the folder structure so that in [programs/single_instruction_tests](/programs/single_instruction_tests/) assembly programs can be written for each type of instruction.

I wrote a [single instruction testbench](/testbench/risc_v/versions/risc_v_single_instructions.h) to assemble each program, instantiate a new instance of the RISC-V CPU, and run the program. Then `TEST_F` would check the output of the CPU against the expected output. The branch-type, jump and upper type tests were then populated in the same way by Arav.

#### Debugging Top Level

As mentioned [joint statement](/statements/readme.md#debugging-top-level-schematic) in the Arav and I worked on debugging the top level together. I was tracing through the design in ISSIE and Arav was checking the values on each wire in GTKWave. This was quite effective in spotting errors in the SystemVerilog.

### Pipelining

I did mostly pair programming in pipelining, only taking partial responsibility for each of the pipelining tasks in our [contributions chart](/README.md#contributions).

#### New top level schematic

Dhyey was tasked with amending the single-cycle schematic to a pipelined one in ISSIE. I later cleaned up this file to clearly show the 5-stage pipeline in the specification [diagram](/rtl/readme.md#pipelined-schematic).



#### Design review of the hazard unit.

- Dhyey and I both referred to the Harris and Harris textbook to understand the hazard unit implementation we need to do.
- Dhyey wrote the [ISSIE schematic](/rtl/hazard_unit/readme.md#schematic) and [specification](/rtl/hazard_unit/readme.md#design-specification-for-the-hazard_unit-module) for the hazard unit.
- I reviewed Dhyey's design using my own notes, and we agreed on the final design.
- Dhyey implemented it in system verilog.

#### Updated top-level

Creating the new registers for pipelining was quite time-consuming, so I used a trick I learnt from my internship to instantiate the modules on the top level schematic easily:
- on vscode, copy the input/output port list of the module.
- paste it on the instantiation, and then use column selection to ctrl+delete `input`, `output` and `[width]` to get the list of ports, then add a `.` at the beginning and `(),` at then end.

Then Sriyesh and Arav connected the wires according to new top-level schematic. 

### Integration testing
I pair-coded with Arav with pipelining integration testing, and we found some errors in the `sign_extend` and `resultSrc` logic. 
Details in the [joint statement](/statements/readme.md#top-level-issues)

### Cache

I drafted an initial change to the top-level to incorporate cache:

![top level schematic with cache](/images/initial-hand-drawn-cache-schematic.png)

Then we worked as a group to try and implement this change. 

We designed and developed a directly mapped write-through cache. While this was not implemented into the pipelined RISC-V processor, we were able to implement the cache into data mem wrapper, where all the testcases passed. See the [joint statement](/statements/readme.md#cache) for more details.

## Reflections

Our group got along very well together, and each person was able to play to their strengths in their individual tasks, and I am proud of the work we have achieved. If I were to do this project again:
- I would be clear on writing specifications for each module before implementation to eliminate design flaws even earlier than the unit tests. This would have helped particularly for the cache.
- Our partially working cache implementation uses a simple replacement policy which is to replace the oldest data. This may not provide the best performance as old data may be used frequently. Hence using a LRU-cache replacement policy with a U-bit would provide better miss-rates.
- We followed a Test-Driven Development approach, which was very effective in ensuring that our code was correct. However, we could have done constrained random-inputs testing to improve our functional coverage.
- We chose to do a lot of pair-coding as the primary purpose of this repo was for learning as much as we can about the RISC-V architecture in a hands on manner. In a practical work setting, we would have used a Gantt chart assigning very specific tasks to each member of the team (such as a verification-only engineer and a separate design-only engineer) to implement parallellism in our work more effectively.
- I would make our port inputs and outputs follow `_i, _o` suffix convention, as this is more consistent with the industry standard.
